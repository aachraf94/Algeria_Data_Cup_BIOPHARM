{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xyRKoP07Fgky",
        "vGh6NUzJSEu1",
        "6w-tN66bTI1q"
<<<<<<< HEAD
      ]
=======
      ],
      "authorship_tag": "ABX9TyOWsGMkvarvggj2ZBE4e1Gm",
      "include_colab_link": true
>>>>>>> 01721f08b1df15a4517dfb92dba59740899c75fb
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
<<<<<<< HEAD
=======
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aachraf94/Algeria_Data_Cup_BIOPHARM/blob/master/IEC_BIOPHARM_Pr%C3%A9diction_des_anomalies_machines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
>>>>>>> 01721f08b1df15a4517dfb92dba59740899c75fb
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Charger les données d'entraînement\n",
        "train_data = pd.read_csv('train_BIOPHARM.csv')\n",
        "\n",
        "# Prétraitement des données\n",
        "le = LabelEncoder()\n",
        "train_data['Event type'] = le.fit_transform(train_data['Event type'])\n",
        "X = train_data.drop(['Index', 'Date', 'Lieu', 'Event type'], axis=1)\n",
        "y = train_data['Event type']\n",
        "\n",
        "# Convertir les valeurs catégorielles en format numérique\n",
        "X = pd.get_dummies(X)\n",
        "\n",
        "# Diviser les données en ensembles supervisés et non supervisés\n",
        "X_train_supervised, X_train_unsupervised, y_train_supervised, _ = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "# Algorithme de clustering semi-supervisé (DBSCAN)\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "X_clustered = dbscan.fit_predict(X_train_unsupervised)\n",
        "\n",
        "# Créer un nouveau DataFrame pour stocker les données clusterisées\n",
        "cluster_df = pd.DataFrame(X_clustered, columns=['Cluster Label'], index=X_train_unsupervised.index)\n",
        "\n",
        "# Fusionner les données clusterisées avec les données supervisées\n",
        "X_train_supervised = X_train_supervised.join(cluster_df)\n",
        "\n",
        "# Entraînement du modèle supervisé (MLPClassifier par exemple)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_supervised.drop('Cluster Label', axis=1))\n",
        "X_test_scaled = scaler.transform(X_train_supervised.drop('Cluster Label', axis=1))\n",
        "\n",
        "clf = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
        "clf.fit(X_train_scaled, y_train_supervised)\n",
        "\n",
        "# Évaluation du modèle\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_train_supervised, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Générer un rapport de classification pour évaluer les performances par classe\n",
        "print(classification_report(y_train_supervised, y_pred))\n",
        "\n",
        "# Charger les données de test pour les prédictions finales\n",
        "test_data = pd.read_csv('test_BIOPHARM.csv')\n",
        "\n",
        "# Convertir les valeurs catégorielles en format numérique pour les données de test\n",
        "test_data = pd.get_dummies(test_data)\n",
        "\n",
        "# Préparer les données de test pour la prédiction\n",
        "X_test = test_data.drop(['Index', 'Date', 'Lieu'], axis=1)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Faire les prédictions finales\n",
        "final_predictions = clf.predict(X_test_scaled)\n",
        "\n",
        "# Créer le fichier de soumission au format requis\n",
        "submission_df = pd.DataFrame(data={'Index': test_data['Index']})\n",
        "for i, column in enumerate(le.classes_):\n",
        "    submission_df[column] = (final_predictions == i).astype(int)\n",
        "\n",
        "submission_df.to_csv('submission_BIOPHARM.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jskUfg4CfVxZ",
        "outputId": "c793e646-d084-4f1e-b123-19860fead051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9825429511018225\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1553\n",
            "           1       1.00      1.00      1.00      2201\n",
            "           2       1.00      1.00      1.00      4049\n",
            "           3       1.00      1.00      1.00       165\n",
            "           4       1.00      1.00      1.00       354\n",
            "           5       1.00      1.00      1.00      4659\n",
            "           6       1.00      1.00      1.00       265\n",
            "           7       1.00      1.00      1.00      2702\n",
            "           8       1.00      1.00      1.00      2367\n",
            "           9       1.00      1.00      1.00        19\n",
            "          10       1.00      1.00      1.00        36\n",
            "          11       0.97      0.45      0.62       830\n",
            "          12       1.00      1.00      1.00        26\n",
            "          13       0.56      1.00      0.72       133\n",
            "          14       0.00      0.00      0.00       103\n",
            "          15       0.97      1.00      0.98     13075\n",
            "\n",
            "    accuracy                           0.98     32537\n",
            "   macro avg       0.91      0.90      0.90     32537\n",
            "weighted avg       0.98      0.98      0.98     32537\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['Index', 'Date', 'Lieu'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-61450ac51862>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Préparer les données de test pour la prédiction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Lieu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mX_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5256\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5257\u001b[0m         \"\"\"\n\u001b[0;32m-> 5258\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5259\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5260\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4547\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4549\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4589\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4590\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4591\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4592\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6699\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{list(labels[mask])} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6700\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6701\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Index', 'Date', 'Lieu'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ejHN8cTd52D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Charger les données d'entraînement\n",
        "train_data = pd.read_csv('train_BIOPHARM.csv')\n",
        "\n",
        "# Prétraitement des données\n",
        "le = LabelEncoder()\n",
        "train_data['Event type'] = le.fit_transform(train_data['Event type'])\n",
        "X = train_data.drop(['Index', 'Date', 'Lieu', 'Event type'], axis=1)\n",
        "y = train_data['Event type']\n",
        "\n",
        "# Convertir les valeurs catégorielles en format numérique\n",
        "X = pd.get_dummies(X)\n",
        "\n",
        "# Diviser les données en ensembles supervisés et non supervisés\n",
        "X_train_supervised, X_train_unsupervised, y_train_supervised, _ = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "# Algorithme de clustering semi-supervisé (DBSCAN)\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "X_clustered = dbscan.fit_predict(X_train_unsupervised)\n",
        "\n",
        "# Créer un nouveau DataFrame pour stocker les données clusterisées\n",
        "cluster_df = pd.DataFrame(X_clustered, columns=['Cluster Label'], index=X_train_unsupervised.index)\n",
        "\n",
        "# Fusionner les données clusterisées avec les données supervisées\n",
        "X_train_supervised = X_train_supervised.join(cluster_df)\n",
        "\n",
        "# Entraînement du modèle supervisé (MLPClassifier par exemple)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_supervised.drop('Cluster Label', axis=1))\n",
        "y_train_supervised = y_train_supervised.astype(int)  # Assurer que les labels sont de type entier pour MLPClassifier\n",
        "\n",
        "clf = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
        "clf.fit(X_train_scaled, y_train_supervised)\n",
        "\n",
        "# Charger les données de test pour les prédictions finales\n",
        "test_data = pd.read_csv('test_BIOPHARM.csv')\n",
        "\n",
        "# Convertir les valeurs catégorielles en format numérique pour les données de test\n",
        "test_data = pd.get_dummies(test_data)\n",
        "\n",
        "# Préparer les données de test pour la prédiction\n",
        "X_test = test_data.drop(['Index', 'Date', 'Lieu'], axis=1)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Faire les prédictions finales\n",
        "final_predictions = clf.predict(X_test_scaled)\n",
        "\n",
        "# Créer le fichier de soumission au format requis\n",
        "submission_df = pd.DataFrame(data={'Index': test_data['Index']})\n",
        "for i, column in enumerate(le.classes_):\n",
        "    submission_df[column] = (final_predictions == i).astype(int)\n",
        "\n",
        "submission_df.to_csv('submission_BIOPHARM.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "bPo0_e0XKtch",
        "outputId": "69654784-508a-4c36-c7b6-f7ca0431ecdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['Index', 'Date', 'Lieu'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0397be947d8d>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Préparer les données de test pour la prédiction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Lieu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mX_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5256\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5257\u001b[0m         \"\"\"\n\u001b[0;32m-> 5258\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5259\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5260\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4547\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4549\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4589\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4590\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4591\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4592\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6699\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{list(labels[mask])} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6700\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6701\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Index', 'Date', 'Lieu'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Chargement des données d'entraînement et de test\n",
        "train_data = pd.read_csv('train_BIOPHARM.csv')\n",
        "test_data = pd.read_csv('test_BIOPHARM.csv')\n",
        "\n",
        "# Conversion des colonnes catégorielles en valeurs numériques\n",
        "labelencoder = LabelEncoder()\n",
        "train_data['Lieu'] = labelencoder.fit_transform(train_data['Lieu'])\n",
        "test_data['Lieu'] = labelencoder.transform(test_data['Lieu'])\n",
        "\n",
        "# Imputation des valeurs manquantes\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "train_data['Valeur'] = imputer.fit_transform(train_data[['Valeur']].values)\n",
        "test_data['Valeur'] = imputer.transform(test_data[['Valeur']].values)\n",
        "\n",
        "# Vérification et suppression des lignes avec des types d'événements non présents dans le jeu d'entraînement\n",
        "unique_labels_train = train_data['Event type'].unique()\n",
        "test_data_filtered = test_data[test_data['Event type'].isin(unique_labels_train)]\n",
        "\n",
        "# Séparation des caractéristiques et de la cible pour les données de test filtrées\n",
        "X_train = train_data.drop('Event type', axis=1)\n",
        "y_train = train_data['Event type']\n",
        "X_test_filtered = test_data_filtered.drop('Index', axis=1)\n",
        "\n",
        "# Création et entraînement du modèle de classification\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Prédiction des événements pour les données de test filtrées\n",
        "y_pred_filtered = model.predict_proba(X_test_filtered)\n",
        "\n",
        "# Création du fichier de soumission pour les données de test filtrées\n",
        "submission_df_filtered = pd.DataFrame({'Index': test_data_filtered['Index']})\n",
        "for event_type in unique_labels_train:\n",
        "    submission_df_filtered[f'Type_{event_type.replace(\" \", \"_\")}'] = y_pred_filtered[:, np.where(unique_labels_train == event_type)[0][0]]\n",
        "\n",
        "# Enregistrement du fichier de soumission pour les données de test filtrées\n",
        "submission_df_filtered.to_csv('submission_filtered_BIOPHARM.csv', index=False)\n"
      ],
      "metadata": {
        "id": "IXVv1WQcQtg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "\n",
        "# Chargement des données\n",
        "train_data = pd.read_csv('train_BIOPHARM.csv')\n",
        "test_data = pd.read_csv('test_BIOPHARM.csv')\n",
        "submission_format = pd.read_csv('sample_submission_BIOPHARM.csv')\n",
        "\n",
        "# Vérification des noms de colonnes réels\n",
        "print(train_data.columns)\n",
        "\n",
        "# Prétraitement des données\n",
        "X = train_data.drop(['Index', 'Type d'événement'], axis=1)\n",
        "y = train_data['Type d'événement']\n",
        "\n",
        "# Encodage des variables catégorielles\n",
        "X_encoded = pd.get_dummies(X)\n",
        "\n",
        "# Séparation des données en ensembles d'entraînement et de validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entraînement du modèle XGBoost\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Évaluation du modèle sur l'ensemble de validation\n",
        "y_pred = model.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "logloss = log_loss(y_val, model.predict_proba(X_val))\n",
        "print(f\"Accuracy: {accuracy}, Log Loss: {logloss}\")\n",
        "\n",
        "# Prédictions sur les données de test et création du fichier de soumission\n",
        "X_test = test_data.drop(['Index'], axis=1)\n",
        "X_test_encoded = pd.get_dummies(X_test)\n",
        "predictions = model.predict_proba(X_test_encoded)\n",
        "\n",
        "submission = pd.DataFrame(predictions, columns=submission_format.columns[1:])\n",
        "submission.insert(0, 'Index', test_data['Index'])\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "sW74xiM8UCJy",
        "outputId": "950e0f78-6470-403b-9f37-143c67fb1366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 15) (<ipython-input-13-004a49a9e9f3>, line 15)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-004a49a9e9f3>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    X = train_data.drop(['Index', 'Type d'événement'], axis=1)\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Charger les données d'entraînement\n",
        "train_data = pd.read_csv('train_BIOPHARM.csv')\n",
        "\n",
        "# Prétraitement des données d'entraînement\n",
        "le = LabelEncoder()\n",
        "train_data['Event type'] = le.fit_transform(train_data['Event type'])\n",
        "X = train_data.drop(['Index', 'Date', 'Lieu', 'Event type'], axis=1)\n",
        "y = train_data['Event type']\n",
        "X = pd.get_dummies(X)\n",
        "\n",
        "# Diviser les données en ensembles supervisés et non supervisés\n",
        "X_train_supervised, X_train_unsupervised, y_train_supervised, _ = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "# Algorithme de clustering semi-supervisé (DBSCAN)\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "X_clustered = dbscan.fit_predict(X_train_unsupervised)\n",
        "\n",
        "# Créer un nouveau DataFrame pour stocker les données clusterisées\n",
        "cluster_df = pd.DataFrame(X_clustered, columns=['Cluster Label'], index=X_train_unsupervised.index)\n",
        "\n",
        "# Fusionner les données clusterisées avec les données supervisées\n",
        "X_train_supervised = X_train_supervised.join(cluster_df)\n",
        "\n",
        "# Entraînement du modèle supervisé (MLPClassifier par exemple)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_supervised.drop('Cluster Label', axis=1))\n",
        "X_test_scaled = scaler.transform(X_train_supervised.drop('Cluster Label', axis=1))\n",
        "\n",
        "clf = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
        "clf.fit(X_train_scaled, y_train_supervised)\n",
        "\n",
        "# Évaluation du modèle\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_train_supervised, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Générer un rapport de classification pour évaluer les performances par classe\n",
        "print(classification_report(y_train_supervised, y_pred))\n",
        "\n",
        "# Charger les données de test pour les prédictions finales\n",
        "test_data = pd.read_csv('test_BIOPHARM.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OfDvHwMd-AH",
        "outputId": "916d78cb-52e1-403d-e96e-0af1598e4aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9825429511018225\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1553\n",
            "           1       1.00      1.00      1.00      2201\n",
            "           2       1.00      1.00      1.00      4049\n",
            "           3       1.00      1.00      1.00       165\n",
            "           4       1.00      1.00      1.00       354\n",
            "           5       1.00      1.00      1.00      4659\n",
            "           6       1.00      1.00      1.00       265\n",
            "           7       1.00      1.00      1.00      2702\n",
            "           8       1.00      1.00      1.00      2367\n",
            "           9       1.00      1.00      1.00        19\n",
            "          10       1.00      1.00      1.00        36\n",
            "          11       0.97      0.45      0.62       830\n",
            "          12       1.00      1.00      1.00        26\n",
            "          13       0.56      1.00      0.72       133\n",
            "          14       0.00      0.00      0.00       103\n",
            "          15       0.97      1.00      0.98     13075\n",
            "\n",
            "    accuracy                           0.98     32537\n",
            "   macro avg       0.91      0.90      0.90     32537\n",
            "weighted avg       0.98      0.98      0.98     32537\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copie des données de test pour éviter de modifier les données originales\n",
        "test_data_processed = test_data.copy()\n",
        "\n",
        "# Création manuelle de la colonne 'Event type' avec une valeur par défaut, par exemple, 'Unknown'\n",
        "test_data_processed['Event type'] = 'Unknown'\n",
        "\n",
        "# Création des variables factices dans les données de test\n",
        "test_data_processed = pd.get_dummies(test_data_processed)\n",
        "\n",
        "# Vérification des colonnes dans les données de test après prétraitement\n",
        "if set(test_data_processed.columns) == set(X.columns):\n",
        "    # Appliquer la transformation avec le même scaler\n",
        "    X_test_scaled = scaler.transform(test_data_processed)\n",
        "    # Faire les prédictions finales et générer le fichier de soumission\n",
        "    final_predictions = clf.predict(X_test_scaled)\n",
        "    submission_df = pd.DataFrame(data={'Index': test_data['Index']})\n",
        "    for i, column in enumerate(le.classes_):\n",
        "        submission_df[column] = (final_predictions == i).astype(int)\n",
        "    submission_df.to_csv('submission_BIOPHARM.csv', index=False)\n",
        "else:\n",
        "    print(\"Les colonnes dans les données de test ne correspondent pas aux colonnes dans les données d'entraînement après prétraitement.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bjE-_OokcS9",
        "outputId": "48518637-4b13-41a4-9b79-2d76b1eedaec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les colonnes dans les données de test ne correspondent pas aux colonnes dans les données d'entraînement après prétraitement.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**l'algorithme de classification supervisée SVM**"
      ],
      "metadata": {
        "id": "c63wolCxRBWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Charger les données d'entraînement et de test\n",
        "train_data = pd.read_csv('train_BIOPHARM.csv')\n",
        "test_data = pd.read_csv('test_BIOPHARM.csv')\n",
        "\n",
        "# Prétraitement des données\n",
        "label_encoder = LabelEncoder()\n",
        "scaler = StandardScaler()\n",
        "imputer_numeric = SimpleImputer(strategy='mean')  # Remplacement des valeurs numériques manquantes par la moyenne\n",
        "imputer_categorical = SimpleImputer(strategy='constant', fill_value='Unknown')  # Remplacement des valeurs catégorielles manquantes par 'Unknown'\n",
        "\n",
        "# Encoder les variables catégorielles\n",
        "train_data['Event_type_encoded'] = label_encoder.fit_transform(train_data['Event type'])\n",
        "\n",
        "# Sélection des caractéristiques pertinentes (par exemple, exclure l'index, la date, etc.)\n",
        "features = ['Valeur', 'Nouvelle valeur', 'Ancienne valeur', 'Event_type_encoded']\n",
        "X = train_data[features]\n",
        "y = train_data['Event type']\n",
        "\n",
        "# Diviser les données en ensembles d'entraînement et de validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Imputation des valeurs numériques manquantes\n",
        "X_train_imputed_numeric = imputer_numeric.fit_transform(X_train.select_dtypes(include=['number']))\n",
        "X_val_imputed_numeric = imputer_numeric.transform(X_val.select_dtypes(include=['number']))\n",
        "\n",
        "# Imputation des valeurs catégorielles manquantes\n",
        "X_train_imputed_categorical = imputer_categorical.fit_transform(X_train.select_dtypes(exclude=['number']))\n",
        "X_val_imputed_categorical = imputer_categorical.transform(X_val.select_dtypes(exclude=['number']))\n",
        "\n",
        "# Concaténer les données imputées\n",
        "X_train_imputed = pd.DataFrame(data=X_train_imputed_numeric, columns=X_train.select_dtypes(include=['number']).columns, index=X_train.index)\n",
        "X_val_imputed = pd.DataFrame(data=X_val_imputed_numeric, columns=X_val.select_dtypes(include=['number']).columns, index=X_val.index)\n",
        "X_train_imputed[X_train.select_dtypes(exclude=['number']).columns] = X_train_imputed_categorical\n",
        "X_val_imputed[X_val.select_dtypes(exclude=['number']).columns] = X_val_imputed_categorical\n",
        "\n",
        "# Normaliser les données\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "X_val_scaled = scaler.transform(X_val_imputed)\n",
        "\n",
        "# Entraîner le modèle SVM\n",
        "svm_model = SVC(kernel='rbf', C=1.0, probability=True)  # Exemple avec un noyau RBF\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Prédiction sur les données de validation\n",
        "y_val_pred = svm_model.predict(X_val_scaled)\n",
        "\n",
        "# Évaluer les performances du modèle\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f\"Accuracy on validation set: {accuracy}\")\n",
        "\n",
        "# Prédiction sur les données de test\n",
        "test_data['Event_type_encoded'] = label_encoder.transform(test_data['Event type'])\n",
        "X_test_numeric = test_data[features].select_dtypes(include=['number'])\n",
        "X_test_imputed_numeric = imputer_numeric.transform(X_test_numeric)\n",
        "X_test_categorical = test_data[features].select_dtypes(exclude=['number'])\n",
        "X_test_imputed_categorical = imputer_categorical.transform(X_test_categorical)\n",
        "X_test_imputed = pd.DataFrame(data=X_test_imputed_numeric, columns=X_test_numeric.columns, index=X_test_numeric.index)\n",
        "X_test_imputed[X_test_categorical.columns] = X_test_imputed_categorical\n",
        "X_test_scaled = scaler.transform(X_test_imputed)\n",
        "test_predictions_proba = svm_model.predict_proba(X_test_scaled)\n",
        "\n",
        "# Créer le fichier de soumission au format demandé\n",
        "submission_df = pd.DataFrame(test_predictions_proba, columns=label_encoder.classes_)\n",
        "submission_df.insert(0, 'Index', test_data['Index'])\n",
        "submission_df.to_csv('Sample_submission_SVM.csv', index=False)\n"
      ],
      "metadata": {
        "id": "eOOuFcDjRMQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **XGBoost**"
      ],
      "metadata": {
        "id": "K44kWMm3Wh67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('cleaned_train_BIOPHARM.csv')\n",
        "test_data = pd.read_csv('test_BIOPHARM.csv')\n",
        "submission_data = pd.read_csv('Sample_submission_BIOPHARM.csv')\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "train_data['Lieu'] = le.fit_transform(train_data['Lieu'])\n",
        "test_data['Lieu'] = le.transform(test_data['Lieu'])\n",
        "\n",
        "# Define features and target\n",
        "features = ['Lieu', 'Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "X_train = train_data[features]\n",
        "y_train = train_data['Event type']\n",
        "X_test = test_data[features]\n",
        "\n",
        "# Initialize and train the XGBoost classifier\n",
        "xgb_classifier = XGBClassifier()\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for each event type in the test data\n",
        "probabilities = xgb_classifier.predict_proba(X_test)\n",
        "\n",
        "# Update the submission file with predicted probabilities for each index\n",
        "for i, index in enumerate(submission_data['Index']):\n",
        "    submission_data.loc[i, submission_data.columns[1:]] = probabilities[i]\n",
        "\n",
        "# Save the updated submission file\n",
        "submission_data.to_csv('Updated_submission_BIOPHARM.csv', index=False)\n"
      ],
      "metadata": {
        "id": "iJoXGy-OXHNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xgboost"
      ],
      "metadata": {
        "id": "LligskFvBb9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('train_BIOPHARM.csv')\n",
        "test_data = pd.read_csv('test_BIOPHARM.csv')\n",
        "submission_data = pd.read_csv('Sample_submission_BIOPHARM.csv')\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Convert categorical columns to codes (integer labels)\n",
        "train_data['Lieu'] = train_data['Lieu'].astype('category').cat.codes\n",
        "test_data['Lieu'] = test_data['Lieu'].astype('category').cat.codes\n",
        "\n",
        "# Encode target variable 'Event type' to integer labels\n",
        "y_train = le.fit_transform(train_data['Event type'])\n",
        "\n",
        "# Convert object dtype columns to numeric\n",
        "numeric_cols = ['Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "train_data[numeric_cols] = train_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "test_data[numeric_cols] = test_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Handle NaN values if any\n",
        "train_data.fillna(0, inplace=True)\n",
        "test_data.fillna(0, inplace=True)\n",
        "\n",
        "# Define features\n",
        "features = ['Lieu', 'Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "X_train = train_data[features]\n",
        "X_test = test_data[features]\n",
        "\n",
        "# Initialize and train the XGBoost classifier\n",
        "xgb_classifier = XGBClassifier()\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for each event type in the test data\n",
        "probabilities = xgb_classifier.predict_proba(X_test)\n",
        "\n",
        "# Update the submission file with predicted probabilities for each index\n",
        "for i, index in enumerate(submission_data['Index']):\n",
        "    submission_data.loc[i, submission_data.columns[1:]] = probabilities[i][:len(submission_data.columns[1:])]\n",
        "\n",
        "# Save the updated submission file\n",
        "submission_data.to_csv('Updated_submission_BIOPHARM.csv', index=False)\n"
      ],
      "metadata": {
        "id": "xYfGLZgQ4igo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Add date field**"
      ],
      "metadata": {
        "id": "xyRKoP07Fgky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('train_BIOPHARM.csv')\n",
        "test_data = pd.read_csv('test_BIOPHARM.csv')\n",
        "submission_data = pd.read_csv('Sample_submission_BIOPHARM.csv')\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Convert categorical columns to codes (integer labels)\n",
        "train_data['Lieu'] = train_data['Lieu'].astype('category').cat.codes\n",
        "test_data['Lieu'] = test_data['Lieu'].astype('category').cat.codes\n",
        "\n",
        "# Encode target variable 'Event type' to integer labels\n",
        "y_train = le.fit_transform(train_data['Event type'])\n",
        "\n",
        "# Convert object dtype columns to numeric\n",
        "numeric_cols = ['Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "train_data[numeric_cols] = train_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "test_data[numeric_cols] = test_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Handle NaN values if any\n",
        "train_data.fillna(0, inplace=True)\n",
        "test_data.fillna(0, inplace=True)\n",
        "\n",
        "# Extract features from the 'Date' column\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "train_data['Year'] = train_data['Date'].dt.year\n",
        "train_data['Month'] = train_data['Date'].dt.month\n",
        "train_data['Day'] = train_data['Date'].dt.day\n",
        "train_data['Hour'] = train_data['Date'].dt.hour\n",
        "train_data['Minute'] = train_data['Date'].dt.minute\n",
        "\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "test_data['Year'] = test_data['Date'].dt.year\n",
        "test_data['Month'] = test_data['Date'].dt.month\n",
        "test_data['Day'] = test_data['Date'].dt.day\n",
        "test_data['Hour'] = test_data['Date'].dt.hour\n",
        "test_data['Minute'] = test_data['Date'].dt.minute\n",
        "\n",
        "# Define features including extracted date features\n",
        "features = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Lieu', 'Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "X_train = train_data[features]\n",
        "X_test = test_data[features]\n",
        "\n",
        "# Initialize and train the XGBoost classifier\n",
        "xgb_classifier = XGBClassifier()\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for each event type in the test data\n",
        "probabilities = xgb_classifier.predict_proba(X_test)\n",
        "\n",
        "# Update the submission file with predicted probabilities for each index\n",
        "for i, index in enumerate(submission_data['Index']):\n",
        "    submission_data.loc[i, submission_data.columns[1:]] = probabilities[i][:len(submission_data.columns[1:])]\n",
        "\n",
        "# Save the updated submission file\n",
        "submission_data.to_csv('Updated_submission_BIOPHARM.csv', index=False)\n"
      ],
      "metadata": {
        "id": "rWrvDPyRFZQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Add date feild**"
      ],
      "metadata": {
        "id": "vGh6NUzJSEu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('train_BIOPHARM.csv')\n",
        "test_data = pd.read_csv('test_BIOPHARM.csv')\n",
        "submission_data = pd.read_csv('Sample_submission_BIOPHARM.csv')\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Convert categorical columns to codes (integer labels)\n",
        "train_data['Lieu'] = train_data['Lieu'].astype('category').cat.codes\n",
        "test_data['Lieu'] = test_data['Lieu'].astype('category').cat.codes\n",
        "\n",
        "# Encode target variable 'Event type' to integer labels\n",
        "y_train = le.fit_transform(train_data['Event type'])\n",
        "\n",
        "# Convert object dtype columns to numeric\n",
        "numeric_cols = ['Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "train_data[numeric_cols] = train_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "test_data[numeric_cols] = test_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Handle NaN values if any\n",
        "train_data.fillna(0, inplace=True)\n",
        "test_data.fillna(0, inplace=True)\n",
        "\n",
        "# Extract features from the 'Date' column\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "train_data['Year'] = train_data['Date'].dt.year\n",
        "train_data['Month'] = train_data['Date'].dt.month\n",
        "train_data['Day'] = train_data['Date'].dt.day\n",
        "train_data['Hour'] = train_data['Date'].dt.hour\n",
        "train_data['Minute'] = train_data['Date'].dt.minute\n",
        "\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "test_data['Year'] = test_data['Date'].dt.year\n",
        "test_data['Month'] = test_data['Date'].dt.month\n",
        "test_data['Day'] = test_data['Date'].dt.day\n",
        "test_data['Hour'] = test_data['Date'].dt.hour\n",
        "test_data['Minute'] = test_data['Date'].dt.minute\n",
        "\n",
        "# Define features including extracted date features\n",
        "features = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Lieu', 'Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "X_train = train_data[features]\n",
        "X_test = test_data[features]\n",
        "\n",
        "# Initialize and train the XGBoost classifier\n",
        "xgb_classifier = XGBClassifier()\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for each event type in the test data\n",
        "probabilities = xgb_classifier.predict_proba(X_test)\n",
        "\n",
        "# Apply softmax transformation to probabilities\n",
        "softmax_probabilities = probabilities / probabilities.sum(axis=1)[:, None]\n",
        "\n",
        "# Update the submission file with softmax probabilities for each index\n",
        "for i, index in enumerate(submission_data['Index']):\n",
        "    submission_data.loc[i, submission_data.columns[1:]] = softmax_probabilities[i][:len(submission_data.columns[1:])]\n",
        "\n",
        "# Scale probabilities to ensure no exact zeros\n",
        "scaler = MinMaxScaler()\n",
        "scaled_probabilities = scaler.fit_transform(softmax_probabilities)\n",
        "\n",
        "# Update the submission file with scaled probabilities for each index\n",
        "for i, index in enumerate(submission_data['Index']):\n",
        "    submission_data.loc[i, submission_data.columns[1:]] = scaled_probabilities[i][:len(submission_data.columns[1:])]\n",
        "\n",
        "# Save the updated submission file\n",
        "submission_data.to_csv('Updated_submission_BIOPHARM__1.csv', index=False)\n"
      ],
      "metadata": {
        "id": "BcbYch5LO_kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code to correct the probabilities**"
      ],
      "metadata": {
        "id": "HYgMG7lpROGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load the updated submission file with predicted probabilities\n",
        "submission_data = pd.read_csv('Updated_submission_BIOPHARM__3_RandForests.csv')\n",
        "\n",
        "# Extract the predicted probabilities columns\n",
        "prob_columns = submission_data.columns[1:]\n",
        "\n",
        "# Apply softmax transformation to probabilities\n",
        "probabilities = submission_data[prob_columns].values\n",
        "softmax_probabilities = probabilities / probabilities.sum(axis=1)[:, None]\n",
        "\n",
        "# Scale probabilities to ensure no exact zeros\n",
        "scaler = MinMaxScaler()\n",
        "scaled_probabilities = scaler.fit_transform(softmax_probabilities)\n",
        "\n",
        "# Update the submission file with scaled probabilities for each index\n",
        "for i, index in enumerate(submission_data['Index']):\n",
        "    submission_data.loc[i, prob_columns] = scaled_probabilities[i]\n",
        "\n",
        "# Save the updated submission file\n",
        "submission_data.to_csv('Updated_submission_BIOPHARM__3_RandForests_corrected.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMwxaQKJRUNl",
        "outputId": "3756974e-eb24-40ee-9a55-9a2d9a07a0d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-1b6b6fee3b78>:12: RuntimeWarning: invalid value encountered in divide\n",
            "  softmax_probabilities = probabilities / probabilities.sum(axis=1)[:, None]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Apply the proba caractiristiqs on the first working code (without date)**"
      ],
      "metadata": {
        "id": "6w-tN66bTI1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import brier_score_loss\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('train_BIOPHARM.csv')\n",
        "test_data = pd.read_csv('test_BIOPHARM.csv')\n",
        "submission_data = pd.read_csv('Sample_submission_BIOPHARM.csv')\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Convert categorical columns to codes (integer labels)\n",
        "train_data['Lieu'] = train_data['Lieu'].astype('category').cat.codes\n",
        "test_data['Lieu'] = test_data['Lieu'].astype('category').cat.codes\n",
        "\n",
        "# Encode target variable 'Event type' to integer labels\n",
        "y_train = le.fit_transform(train_data['Event type'])\n",
        "\n",
        "# Convert object dtype columns to numeric\n",
        "numeric_cols = ['Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "train_data[numeric_cols] = train_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "test_data[numeric_cols] = test_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Handle NaN values if any\n",
        "train_data.fillna(0, inplace=True)\n",
        "test_data.fillna(0, inplace=True)\n",
        "\n",
        "# Define features\n",
        "features = ['Lieu', 'Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "X_train = train_data[features]\n",
        "X_test = test_data[features]\n",
        "\n",
        "# Initialize and train the XGBoost classifier\n",
        "xgb_classifier = XGBClassifier()\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for each event type in the test data\n",
        "probabilities = xgb_classifier.predict_proba(X_test)\n",
        "\n",
        "# Apply softmax transformation to probabilities\n",
        "softmax_probabilities = probabilities / probabilities.sum(axis=1)[:, None]\n",
        "\n",
        "# Apply probability calibration using isotonic regression\n",
        "calibrated_classifier = CalibratedClassifierCV(xgb_classifier, cv='prefit', method='isotonic')\n",
        "calibrated_classifier.fit(X_train, y_train)\n",
        "calibrated_probabilities = calibrated_classifier.predict_proba(X_test)\n",
        "\n",
        "# Update the submission file with calibrated probabilities\n",
        "for i, index in enumerate(submission_data['Index']):\n",
        "    submission_data.loc[i, submission_data.columns[1:]] = calibrated_probabilities[i][:len(submission_data.columns[1:])]\n",
        "\n",
        "# Save the updated submission file\n",
        "submission_data.to_csv('Updated_submission_BIOPHARM__2.csv', index=False)\n",
        "\n",
        "# Evaluate calibration using Brier score\n",
        "X_train_cal, X_valid_cal, y_train_cal, y_valid_cal = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "calibrated_classifier.fit(X_train_cal, y_train_cal)\n",
        "valid_probabilities = calibrated_classifier.predict_proba(X_valid_cal)\n",
        "brier_score = brier_score_loss(y_valid_cal, valid_probabilities[:, 1])  # Assuming binary classification\n",
        "print(f\"Brier Score: {brier_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "GjlfWBmUTcKm",
        "outputId": "8bf91a0a-7cd0-4ede-af48-f32c4136f9ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Only binary classification is supported. The type of the target is multiclass.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e8010768d9e6>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mcalibrated_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_cal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_cal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mvalid_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalibrated_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_cal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mbrier_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrier_score_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid_cal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_probabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming binary classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Brier Score: {brier_score}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mbrier_score_loss\u001b[0;34m(y_true, y_prob, sample_weight, pos_label)\u001b[0m\n\u001b[1;32m   2879\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2880\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2881\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2882\u001b[0m             \u001b[0;34m\"Only binary classification is supported. The type of the target \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2883\u001b[0m             \u001b[0;34mf\"is {y_type}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Only binary classification is supported. The type of the target is multiclass."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Apply Random Forests algo**"
      ],
      "metadata": {
        "id": "aGUQw3rSZGjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('train_BIOPHARM.csv')\n",
        "test_data = pd.read_csv('test_BIOPHARM.csv')\n",
        "submission_data = pd.read_csv('Sample_submission_BIOPHARM.csv')\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Convert categorical columns to codes (integer labels)\n",
        "train_data['Lieu'] = train_data['Lieu'].astype('category').cat.codes\n",
        "test_data['Lieu'] = test_data['Lieu'].astype('category').cat.codes\n",
        "\n",
        "# Encode target variable 'Event type' to integer labels\n",
        "y_train = le.fit_transform(train_data['Event type'])\n",
        "\n",
        "# Convert object dtype columns to numeric\n",
        "numeric_cols = ['Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "train_data[numeric_cols] = train_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "test_data[numeric_cols] = test_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Handle NaN values if any\n",
        "train_data.fillna(0, inplace=True)\n",
        "test_data.fillna(0, inplace=True)\n",
        "\n",
        "# Define features\n",
        "features = ['Lieu', 'Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "X_train = train_data[features]\n",
        "X_test = test_data[features]\n",
        "\n",
        "# Initialize and train the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier()\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for each event type in the test data\n",
        "probabilities = rf_classifier.predict_proba(X_test)\n",
        "\n",
        "# Update the submission file with predicted probabilities for each index\n",
        "for i, index in enumerate(submission_data['Index']):\n",
        "    submission_data.loc[i, submission_data.columns[1:]] = probabilities[i][:len(submission_data.columns[1:])]\n",
        "\n",
        "# Save the updated submission file\n",
        "submission_data.to_csv('Updated_submission_BIOPHARM__3_RandForests.csv', index=False)\n"
      ],
      "metadata": {
        "id": "KIdqfdJ5ZFnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradient Boosting Classifier**"
      ],
      "metadata": {
        "id": "moezyrLwiabn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('train_BIOPHARM.csv')\n",
        "test_data = pd.read_csv('test_BIOPHARM.csv')\n",
        "submission_data = pd.read_csv('Sample_submission_BIOPHARM.csv')\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Convert categorical columns to codes (integer labels)\n",
        "train_data['Lieu'] = train_data['Lieu'].astype('category').cat.codes\n",
        "test_data['Lieu'] = test_data['Lieu'].astype('category').cat.codes\n",
        "\n",
        "# Encode target variable 'Event type' to integer labels\n",
        "y_train = le.fit_transform(train_data['Event type'])\n",
        "\n",
        "# Convert object dtype columns to numeric\n",
        "numeric_cols = ['Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "train_data[numeric_cols] = train_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "test_data[numeric_cols] = test_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Handle NaN values if any\n",
        "train_data.fillna(0, inplace=True)\n",
        "test_data.fillna(0, inplace=True)\n",
        "\n",
        "# Define features\n",
        "features = ['Lieu', 'Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "X_train = train_data[features]\n",
        "X_test = test_data[features]\n",
        "\n",
        "# Initialize and train the Gradient Boosting classifier\n",
        "gb_classifier = GradientBoostingClassifier()\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for each event type in the test data\n",
        "probabilities = gb_classifier.predict_proba(X_test)\n",
        "\n",
        "# Update the submission file with predicted probabilities for each index\n",
        "for i, index in enumerate(submission_data['Index']):\n",
        "    submission_data.loc[i, submission_data.columns[1:]] = probabilities[i][:len(submission_data.columns[1:])]\n",
        "\n",
        "# Save the updated submission file\n",
        "submission_data.to_csv('Updated_submission_BIOPHARM__4_GBC.csv', index=False)\n"
      ],
      "metadata": {
        "id": "9t4rBjhlihEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GBC with the date field**"
      ],
      "metadata": {
        "id": "erdttoXolUej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('train_BIOPHARM.csv')\n",
        "test_data = pd.read_csv('test_BIOPHARM.csv')\n",
        "submission_data = pd.read_csv('Sample_submission_BIOPHARM.csv')\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Convert categorical columns to codes (integer labels)\n",
        "train_data['Lieu'] = train_data['Lieu'].astype('category').cat.codes\n",
        "test_data['Lieu'] = test_data['Lieu'].astype('category').cat.codes\n",
        "\n",
        "# Encode target variable 'Event type' to integer labels\n",
        "y_train = le.fit_transform(train_data['Event type'])\n",
        "\n",
        "# Convert object dtype columns to numeric\n",
        "numeric_cols = ['Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "train_data[numeric_cols] = train_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "test_data[numeric_cols] = test_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Handle NaN values if any\n",
        "train_data.fillna(0, inplace=True)\n",
        "test_data.fillna(0, inplace=True)\n",
        "\n",
        "# Extract features from the 'Date' column\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "train_data['Year'] = train_data['Date'].dt.year\n",
        "train_data['Month'] = train_data['Date'].dt.month\n",
        "train_data['Day'] = train_data['Date'].dt.day\n",
        "train_data['Hour'] = train_data['Date'].dt.hour\n",
        "train_data['Minute'] = train_data['Date'].dt.minute\n",
        "\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "test_data['Year'] = test_data['Date'].dt.year\n",
        "test_data['Month'] = test_data['Date'].dt.month\n",
        "test_data['Day'] = test_data['Date'].dt.day\n",
        "test_data['Hour'] = test_data['Date'].dt.hour\n",
        "test_data['Minute'] = test_data['Date'].dt.minute\n",
        "\n",
        "# Define features including extracted date features\n",
        "features = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Lieu', 'Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "X_train = train_data[features]\n",
        "X_test = test_data[features]\n",
        "\n",
        "# Initialize and train the Gradient Boosting classifier\n",
        "gb_classifier = GradientBoostingClassifier()\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for each event type in the test data\n",
        "probabilities = gb_classifier.predict_proba(X_test)\n",
        "\n",
        "# Update the submission file with predicted probabilities for each index\n",
        "for i, index in enumerate(submission_data['Index']):\n",
        "    submission_data.loc[i, submission_data.columns[1:]] = probabilities[i][:len(submission_data.columns[1:])]\n",
        "\n",
        "# Save the updated submission file\n",
        "submission_data.to_csv('Updated_submission_BIOPHARM__5_GBC.csv', index=False)\n"
      ],
      "metadata": {
        "id": "3eQio08Sld2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('train_BIOPHARM.csv')\n",
        "test_data = pd.read_csv('test_BIOPHARM.csv')\n",
        "submission_data = pd.read_csv('Sample_submission_BIOPHARM.csv')\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Convert categorical columns to codes (integer labels)\n",
        "train_data['Lieu'] = train_data['Lieu'].astype('category').cat.codes\n",
        "test_data['Lieu'] = test_data['Lieu'].astype('category').cat.codes\n",
        "\n",
        "# Encode target variable 'Event type' to integer labels\n",
        "y_train = le.fit_transform(train_data['Event type'])\n",
        "\n",
        "# Convert object dtype columns to numeric\n",
        "numeric_cols = ['Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "train_data[numeric_cols] = train_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "test_data[numeric_cols] = test_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Handle NaN values if any\n",
        "train_data.fillna(0, inplace=True)\n",
        "test_data.fillna(0, inplace=True)\n",
        "\n",
        "# Extract features from the 'Date' column\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "train_data['Year'] = train_data['Date'].dt.year\n",
        "train_data['Month'] = train_data['Date'].dt.month\n",
        "train_data['Day'] = train_data['Date'].dt.day\n",
        "train_data['Hour'] = train_data['Date'].dt.hour\n",
        "train_data['Minute'] = train_data['Date'].dt.minute\n",
        "\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "test_data['Year'] = test_data['Date'].dt.year\n",
        "test_data['Month'] = test_data['Date'].dt.month\n",
        "test_data['Day'] = test_data['Date'].dt.day\n",
        "test_data['Hour'] = test_data['Date'].dt.hour\n",
        "test_data['Minute'] = test_data['Date'].dt.minute\n",
        "\n",
        "# Define features including extracted date features\n",
        "features = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Lieu', 'Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "X_train = train_data[features]\n",
        "X_test = test_data[features]\n",
        "\n",
        "# Initialize and train the Gradient Boosting classifier\n",
        "gb_classifier = GradientBoostingClassifier()\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for each event type in the test data\n",
        "probabilities = gb_classifier.predict_proba(X_test)\n",
        "\n",
        "# Apply softmax transformation to probabilities\n",
        "softmax_probabilities = np.exp(probabilities) / np.sum(np.exp(probabilities), axis=1)[:, np.newaxis]\n",
        "\n",
        "# Update the submission file with softmax probabilities for each index\n",
        "for i, index in enumerate(submission_data['Index']):\n",
        "    submission_data.loc[i, submission_data.columns[1:]] = softmax_probabilities[i][:len(submission_data.columns[1:])]\n",
        "\n",
        "# Save the updated submission file\n",
        "submission_data.to_csv('Updated_submission_BIOPHARM__5_GBC_softmax.csv', index=False)\n"
      ],
      "metadata": {
        "id": "DUxf58pOoNyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Add smothing to Gradient bosting classifier algorithm**"
      ],
      "metadata": {
        "id": "-rPgid--74sp"
      }
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('train_BIOPHARM.csv')\n",
        "test_data = pd.read_csv('test_BIOPHARM.csv')\n",
        "submission_data = pd.read_csv('Sample_submission_BIOPHARM.csv')\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Convert categorical columns to codes (integer labels)\n",
        "train_data['Lieu'] = train_data['Lieu'].astype('category').cat.codes\n",
        "test_data['Lieu'] = test_data['Lieu'].astype('category').cat.codes\n",
        "\n",
        "# Encode target variable 'Event type' to integer labels\n",
        "y_train = le.fit_transform(train_data['Event type'])\n",
        "\n",
        "# Convert object dtype columns to numeric\n",
        "numeric_cols = ['Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "train_data[numeric_cols] = train_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "test_data[numeric_cols] = test_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Handle NaN values if any\n",
        "train_data.fillna(0, inplace=True)\n",
        "test_data.fillna(0, inplace=True)\n",
        "\n",
        "# Extract features from the 'Date' column\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "train_data['Year'] = train_data['Date'].dt.year\n",
        "train_data['Month'] = train_data['Date'].dt.month\n",
        "train_data['Day'] = train_data['Date'].dt.day\n",
        "train_data['Hour'] = train_data['Date'].dt.hour\n",
        "train_data['Minute'] = train_data['Date'].dt.minute\n",
        "\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "test_data['Year'] = test_data['Date'].dt.year\n",
        "test_data['Month'] = test_data['Date'].dt.month\n",
        "test_data['Day'] = test_data['Date'].dt.day\n",
        "test_data['Hour'] = test_data['Date'].dt.hour\n",
        "test_data['Minute'] = test_data['Date'].dt.minute\n",
        "\n",
        "# Define features including extracted date features\n",
        "features = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Lieu', 'Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "X_train = train_data[features]\n",
        "X_test = test_data[features]\n",
        "\n",
        "# Initialize and train the Gradient Boosting classifier\n",
        "gb_classifier = GradientBoostingClassifier()\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for each event type in the test data\n",
        "probabilities = gb_classifier.predict_proba(X_test)\n",
        "\n",
        "# Apply Laplace smoothing to probabilities\n",
        "alpha = 0.01  # Laplace smoothing constant\n",
        "smoothed_probabilities = (probabilities + alpha) / (np.sum(probabilities, axis=1)[:, np.newaxis] + (probabilities.shape[1] * alpha))\n",
        "\n",
        "# Update the submission file with smoothed probabilities for each index\n",
        "for i, index in enumerate(submission_data['Index']):\n",
        "    submission_data.loc[i, submission_data.columns[1:]] = smoothed_probabilities[i][:len(submission_data.columns[1:])]\n",
        "\n",
        "# Save the updated submission file\n",
        "submission_data.to_csv('Updated_submission_BIOPHARM__5_GBC_smoothed_notCleaned.csv', index=False)\n"
      ],
=======
      "source": [],
>>>>>>> 01721f08b1df15a4517dfb92dba59740899c75fb
      "metadata": {
        "id": "-JeBqo-T8BOx"
      },
      "execution_count": null,
      "outputs": []
<<<<<<< HEAD
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Smoothing + softmax**"
      ],
      "metadata": {
        "id": "i9TtTo0_AYMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('train_BIOPHARM.csv')\n",
        "test_data = pd.read_csv('test_BIOPHARM.csv')\n",
        "submission_data = pd.read_csv('Sample_submission_BIOPHARM.csv')\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Convert categorical columns to codes (integer labels)\n",
        "train_data['Lieu'] = train_data['Lieu'].astype('category').cat.codes\n",
        "test_data['Lieu'] = test_data['Lieu'].astype('category').cat.codes\n",
        "\n",
        "# Encode target variable 'Event type' to integer labels\n",
        "y_train = le.fit_transform(train_data['Event type'])\n",
        "\n",
        "# Convert object dtype columns to numeric\n",
        "numeric_cols = ['Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "train_data[numeric_cols] = train_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "test_data[numeric_cols] = test_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Handle NaN values if any\n",
        "train_data.fillna(0, inplace=True)\n",
        "test_data.fillna(0, inplace=True)\n",
        "\n",
        "# Extract features from the 'Date' column\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "train_data['Year'] = train_data['Date'].dt.year\n",
        "train_data['Month'] = train_data['Date'].dt.month\n",
        "train_data['Day'] = train_data['Date'].dt.day\n",
        "train_data['Hour'] = train_data['Date'].dt.hour\n",
        "train_data['Minute'] = train_data['Date'].dt.minute\n",
        "\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "test_data['Year'] = test_data['Date'].dt.year\n",
        "test_data['Month'] = test_data['Date'].dt.month\n",
        "test_data['Day'] = test_data['Date'].dt.day\n",
        "test_data['Hour'] = test_data['Date'].dt.hour\n",
        "test_data['Minute'] = test_data['Date'].dt.minute\n",
        "\n",
        "# Define features including extracted date features\n",
        "features = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Lieu', 'Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "X_train = train_data[features]\n",
        "X_test = test_data[features]\n",
        "\n",
        "# Initialize and train the Gradient Boosting classifier\n",
        "gb_classifier = GradientBoostingClassifier()\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for each event type in the test data\n",
        "probabilities = gb_classifier.predict_proba(X_test)\n",
        "\n",
        "# Apply Laplace smoothing to probabilities\n",
        "alpha = 0.01  # Laplace smoothing constant\n",
        "smoothed_probabilities = (probabilities + alpha) / (np.sum(probabilities, axis=1)[:, np.newaxis] + (probabilities.shape[1] * alpha))\n",
        "\n",
        "# Apply softmax transformation to smoothed probabilities\n",
        "softmax_smoothed_probabilities = np.exp(smoothed_probabilities) / np.sum(np.exp(smoothed_probabilities), axis=1)[:, np.newaxis]\n",
        "\n",
        "# Update the submission file with softmax smoothed probabilities for each index\n",
        "for i, index in enumerate(submission_data['Index']):\n",
        "    submission_data.loc[i, submission_data.columns[1:]] = softmax_smoothed_probabilities[i][:len(submission_data.columns[1:])]\n",
        "\n",
        "# Save the updated submission file\n",
        "submission_data.to_csv('Updated_submission_BIOPHARM__5_GBC_smoothed_softmax_notcleaned.csv', index=False)\n"
      ],
      "metadata": {
        "id": "p8BQdjCMAgvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Grid Search for Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "HkDKNXQ5J1AS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import signal  # Import signal module for custom timeout\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('train_BIOPHARM.csv')\n",
        "test_data = pd.read_csv('test_BIOPHARM.csv')\n",
        "submission_data = pd.read_csv('Sample_submission_BIOPHARM.csv')\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Convert categorical columns to codes (integer labels)\n",
        "train_data['Lieu'] = train_data['Lieu'].astype('category').cat.codes\n",
        "test_data['Lieu'] = test_data['Lieu'].astype('category').cat.codes\n",
        "\n",
        "# Encode target variable 'Event type' to integer labels\n",
        "y_train = le.fit_transform(train_data['Event type'])\n",
        "\n",
        "# Convert object dtype columns to numeric\n",
        "numeric_cols = ['Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "train_data[numeric_cols] = train_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "test_data[numeric_cols] = test_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Handle NaN values if any\n",
        "train_data.fillna(0, inplace=True)\n",
        "test_data.fillna(0, inplace=True)\n",
        "\n",
        "# Extract features from the 'Date' column\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "train_data['Year'] = train_data['Date'].dt.year\n",
        "train_data['Month'] = train_data['Date'].dt.month\n",
        "train_data['Day'] = train_data['Date'].dt.day\n",
        "train_data['Hour'] = train_data['Date'].dt.hour\n",
        "train_data['Minute'] = train_data['Date'].dt.minute\n",
        "\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "test_data['Year'] = test_data['Date'].dt.year\n",
        "test_data['Month'] = test_data['Date'].dt.month\n",
        "test_data['Day'] = test_data['Date'].dt.day\n",
        "test_data['Hour'] = test_data['Date'].dt.hour\n",
        "test_data['Minute'] = test_data['Date'].dt.minute\n",
        "\n",
        "# Define features including extracted date features\n",
        "features = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Lieu', 'Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "X_train = train_data[features]\n",
        "X_test = test_data[features]\n",
        "\n",
        "# Initialize the Gradient Boosting classifier\n",
        "gb_classifier = GradientBoostingClassifier()\n",
        "\n",
        "# Define hyperparameters grid for Grid Search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}\n",
        "\n",
        "# Custom function for timeout\n",
        "def timeout_handler(signum, frame):\n",
        "    raise TimeoutError(\"Grid Search took too long and was terminated.\")\n",
        "\n",
        "# Set the custom timeout for Grid Search\n",
        "signal.signal(signal.SIGALRM, timeout_handler)\n",
        "signal.alarm(1800)  # 1800 seconds (30 minutes)\n",
        "\n",
        "try:\n",
        "    # Initialize Grid Search with the classifier and hyperparameters grid\n",
        "    grid_search = GridSearchCV(gb_classifier, param_grid, cv=5, n_jobs=-1, verbose=1, error_score='raise')\n",
        "\n",
        "    # Train the model using Grid Search\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Best hyperparameters found by Grid Search\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Initialize the Gradient Boosting classifier with the best hyperparameters\n",
        "    gb_classifier = GradientBoostingClassifier(**best_params)\n",
        "\n",
        "    # Train the model with the entire training data\n",
        "    gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Predict probabilities for each event type in the test data\n",
        "    probabilities = gb_classifier.predict_proba(X_test)\n",
        "\n",
        "    # Apply Laplace smoothing to probabilities\n",
        "    alpha = 0.01  # Laplace smoothing constant\n",
        "    smoothed_probabilities = (probabilities + alpha) / (probabilities.sum(axis=1)[:, np.newaxis] + (probabilities.shape[1] * alpha))\n",
        "\n",
        "    # Apply softmax transformation to smoothed probabilities\n",
        "    softmax_smoothed_probabilities = np.exp(smoothed_probabilities) / np.sum(np.exp(smoothed_probabilities), axis=1)[:, np.newaxis]\n",
        "\n",
        "    # Update the submission file with softmax smoothed probabilities for each index\n",
        "    for i, index in enumerate(submission_data['Index']):\n",
        "        submission_data.loc[i, submission_data.columns[1:]] = softmax_smoothed_probabilities[i]\n",
        "\n",
        "    # Save the updated submission file\n",
        "    submission_data.to_csv('Updated_submission_BIOPHARM__5_GBC_smoothed_softmax_notcleaned.csv', index=False)\n",
        "    print(\"File is ready\")\n",
        "\n",
        "except TimeoutError as e:\n",
        "    print(e)\n",
        "\n",
        "finally:\n",
        "    signal.alarm(0)  # Reset the alarm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHnp_Ub0KIsR",
        "outputId": "020ccb9e-09cf-49ad-ba4e-da679a652cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
            "Grid Search took too long and was terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import signal  # Import signal module for custom timeout\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('train_BIOPHARM.csv')\n",
        "test_data = pd.read_csv('test_BIOPHARM.csv')\n",
        "submission_data = pd.read_csv('Sample_submission_BIOPHARM.csv')\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Convert categorical columns to codes (integer labels)\n",
        "train_data['Lieu'] = train_data['Lieu'].astype('category').cat.codes\n",
        "test_data['Lieu'] = test_data['Lieu'].astype('category').cat.codes\n",
        "\n",
        "# Encode target variable 'Event type' to integer labels\n",
        "y_train = le.fit_transform(train_data['Event type'])\n",
        "\n",
        "# Convert object dtype columns to numeric\n",
        "numeric_cols = ['Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "train_data[numeric_cols] = train_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "test_data[numeric_cols] = test_data[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Handle NaN values if any\n",
        "train_data.fillna(0, inplace=True)\n",
        "test_data.fillna(0, inplace=True)\n",
        "\n",
        "# Extract features from the 'Date' column\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "train_data['Year'] = train_data['Date'].dt.year\n",
        "train_data['Month'] = train_data['Date'].dt.month\n",
        "train_data['Day'] = train_data['Date'].dt.day\n",
        "train_data['Hour'] = train_data['Date'].dt.hour\n",
        "train_data['Minute'] = train_data['Date'].dt.minute\n",
        "\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "test_data['Year'] = test_data['Date'].dt.year\n",
        "test_data['Month'] = test_data['Date'].dt.month\n",
        "test_data['Day'] = test_data['Date'].dt.day\n",
        "test_data['Hour'] = test_data['Date'].dt.hour\n",
        "test_data['Minute'] = test_data['Date'].dt.minute\n",
        "\n",
        "# Define features including extracted date features\n",
        "features = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Lieu', 'Valeur', 'Nouvelle valeur', 'Ancienne valeur']\n",
        "X_train = train_data[features]\n",
        "X_test = test_data[features]\n",
        "\n",
        "# Initialize the Gradient Boosting classifier\n",
        "gb_classifier = GradientBoostingClassifier()\n",
        "\n",
        "# Define the hyperparameter distributions for Randomized Search\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 150),\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'max_depth': randint(3, 7),\n",
        "    'min_samples_split': randint(2, 10),\n",
        "}\n",
        "\n",
        "# Initialize Randomized Search with the classifier and hyperparameter distributions\n",
        "random_search = RandomizedSearchCV(gb_classifier, param_distributions=param_dist, n_iter=10, cv=5, verbose=1, n_jobs=-1)\n",
        "\n",
        "# Train the model using Randomized Search\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters found by Randomized Search\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "# Initialize the Gradient Boosting classifier with the best hyperparameters\n",
        "gb_classifier = GradientBoostingClassifier(**best_params)\n",
        "\n",
        "# Train the model with the entire training data\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for each event type in the test data\n",
        "probabilities = gb_classifier.predict_proba(X_test)\n",
        "\n",
        "# Apply Laplace smoothing to probabilities (same as before)\n",
        "alpha = 0.01  # Laplace smoothing constant\n",
        "smoothed_probabilities = (probabilities + alpha) / (probabilities.sum(axis=1)[:, np.newaxis] + (probabilities.shape[1] * alpha))\n",
        "\n",
        "# Apply softmax transformation to smoothed probabilities\n",
        "softmax_smoothed_probabilities = np.exp(smoothed_probabilities) / np.sum(np.exp(smoothed_probabilities), axis=1)[:, np.newaxis]\n",
        "\n",
        "# Update the submission file with softmax smoothed probabilities for each index\n",
        "for i, index in enumerate(submission_data['Index']):\n",
        "    submission_data.loc[i, submission_data.columns[1:]] = softmax_smoothed_probabilities[i][:len(submission_data.columns[1:])]\n",
        "\n",
        "# Save the updated submission file\n",
        "submission_data.to_csv('Updated_submission_BIOPHARM__5_GBC_smoothed_softmax_notcleaned.csv', index=False)\n",
        "print(\"File is ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci-R_jiftCSO",
        "outputId": "7394a90b-ea2c-48f6-8f0e-237d78a5c49a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "File is ready\n"
          ]
        }
      ]
=======
>>>>>>> 01721f08b1df15a4517dfb92dba59740899c75fb
    }
  ]
}